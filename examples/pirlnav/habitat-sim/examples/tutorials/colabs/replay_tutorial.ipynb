{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gfx Replay Tutorial\n",
    "\n",
    "gfx replay is a feature that lets you save the visual state of the sim, restore the visual state later, and either reproduce earlier observations (from the same camera position) or produce new observations (from different camera positions).\n",
    "\n",
    "Note that restoring the visual state is not the same as restoring the full simulation state. When playing a replay, no physics or other simulation is running. Object ids from the earlier recorded simulation aren't valid and you can't interact with objects in the scene. However, you can move your agent, sensor, or camera to produce new observations from different camera positions.\n",
    "\n",
    "The recording API:\n",
    "- cfg.enable_gfx_replay_save\n",
    "- sim.gfx_replay_manager.save_keyframe\n",
    "- gfx_replay_utils.add_node_user_transform (wraps sim.gfx_replay_manager.add_user_transform_to_keyframe)\n",
    "- sim.gfx_replay_manager.write_saved_keyframes_to_file\n",
    "\n",
    "The playback API:\n",
    "- gfx_replay_utils.make_backend_configuration_for_playback\n",
    "- sim.gfx_replay_manager.read_keyframes_from_file\n",
    "- player.set_keyframe_index\n",
    "- player.get_user_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L https://raw.githubusercontent.com/facebookresearch/habitat-sim/main/examples/colab_utils/colab_install.sh | NIGHTLY=true bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%cd /content/habitat-sim\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import git\n",
    "import magnum as mn\n",
    "import numpy as np\n",
    "\n",
    "import habitat_sim\n",
    "from habitat_sim.gfx import LightInfo, LightPositionModel\n",
    "from habitat_sim.utils import gfx_replay_utils\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/usr/bin/ffmpeg\"\n",
    "\n",
    "repo = git.Repo(\".\", search_parent_directories=True)\n",
    "dir_path = repo.working_tree_dir\n",
    "%cd $dir_path\n",
    "data_path = os.path.join(dir_path, \"data\")\n",
    "output_path = os.path.join(dir_path, \"examples/tutorials/replay_tutorial_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Configure sim, including enable_gfx_replay_save flag.\n",
    "This flag is required in order to use the gfx replay recording API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_configuration(settings):\n",
    "    make_video_during_sim = False\n",
    "    if \"make_video_during_sim\" in settings:\n",
    "        make_video_during_sim = settings[\"make_video_during_sim\"]\n",
    "\n",
    "    # simulator configuration\n",
    "    backend_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    backend_cfg.scene_id = os.path.join(\n",
    "        data_path, \"scene_datasets/habitat-test-scenes/apartment_1.glb\"\n",
    "    )\n",
    "    assert os.path.exists(backend_cfg.scene_id)\n",
    "    backend_cfg.enable_physics = True\n",
    "\n",
    "    # Enable gfx replay save. See also our call to sim.gfx_replay_manager.save_keyframe()\n",
    "    # below.\n",
    "    backend_cfg.enable_gfx_replay_save = True\n",
    "    backend_cfg.create_renderer = make_video_during_sim\n",
    "\n",
    "    sensor_cfg = habitat_sim.CameraSensorSpec()\n",
    "    sensor_cfg.resolution = [544, 720]\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = [sensor_cfg]\n",
    "\n",
    "    return habitat_sim.Configuration(backend_cfg, [agent_cfg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Helper function to move our agent, step physics, and showcase the replay recording API.\n",
    "Note calls to gfx_replay_utils.add_node_user_transform and sim.gfx_replay_manager.save_keyframe()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def simulate_with_moving_agent(\n",
    "    sim,\n",
    "    duration=1.0,\n",
    "    agent_vel=np.array([0, 0, 0]),\n",
    "    look_rotation_vel=0.0,\n",
    "    get_frames=True,\n",
    "):\n",
    "    sensor_node = sim._sensors[\"rgba_camera\"]._sensor_object.object\n",
    "    agent_node = sim.get_agent(0).body.object\n",
    "\n",
    "    # simulate dt seconds at 60Hz to the nearest fixed timestep\n",
    "    time_step = 1.0 / 60.0\n",
    "\n",
    "    rotation_x = mn.Quaternion.rotation(\n",
    "        mn.Deg(look_rotation_vel) * time_step, mn.Vector3(1.0, 0, 0)\n",
    "    )\n",
    "\n",
    "    print(\"Simulating \" + str(duration) + \" world seconds.\")\n",
    "    observations = []\n",
    "    start_time = sim.get_world_time()\n",
    "    while sim.get_world_time() < start_time + duration:\n",
    "\n",
    "        # move agent\n",
    "        agent_node.translation += agent_vel * time_step\n",
    "\n",
    "        # rotate sensor\n",
    "        sensor_node.rotation *= rotation_x\n",
    "\n",
    "        # Add user transforms for the agent and sensor. We'll use these later during\n",
    "        # replay playback.\n",
    "        gfx_replay_utils.add_node_user_transform(sim, agent_node, \"agent\")\n",
    "        gfx_replay_utils.add_node_user_transform(sim, sensor_node, \"sensor\")\n",
    "\n",
    "        sim.step_physics(time_step)\n",
    "\n",
    "        # save a replay keyframe after every physics step\n",
    "        sim.gfx_replay_manager.save_keyframe()\n",
    "\n",
    "        if get_frames:\n",
    "            observations.append(sim.get_sensor_observations())\n",
    "\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## More tutorial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def configure_lighting(sim):\n",
    "    light_setup = [\n",
    "        LightInfo(\n",
    "            vector=[1.0, 1.0, 0.0, 1.0],\n",
    "            color=[18.0, 18.0, 18.0],\n",
    "            model=LightPositionModel.Global,\n",
    "        ),\n",
    "        LightInfo(\n",
    "            vector=[0.0, -1.0, 0.0, 1.0],\n",
    "            color=[5.0, 5.0, 5.0],\n",
    "            model=LightPositionModel.Global,\n",
    "        ),\n",
    "        LightInfo(\n",
    "            vector=[-1.0, 1.0, 1.0, 1.0],\n",
    "            color=[18.0, 18.0, 18.0],\n",
    "            model=LightPositionModel.Global,\n",
    "        ),\n",
    "    ]\n",
    "    sim.set_light_setup(light_setup)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--no-show-video\", dest=\"show_video\", action=\"store_false\")\n",
    "    parser.add_argument(\"--no-make-video\", dest=\"make_video\", action=\"store_false\")\n",
    "    parser.set_defaults(show_video=True, make_video=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    show_video = args.show_video\n",
    "    make_video = args.make_video\n",
    "    make_video_during_sim = False\n",
    "else:\n",
    "    show_video = False\n",
    "    make_video = False\n",
    "\n",
    "if make_video and not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "cfg = make_configuration({\"make_video_during_sim\": make_video_during_sim})\n",
    "sim = None\n",
    "replay_filepath = \"./replay.json\"\n",
    "\n",
    "if not sim:\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "else:\n",
    "    sim.reconfigure(cfg)\n",
    "\n",
    "configure_lighting(sim)\n",
    "\n",
    "agent_state = habitat_sim.AgentState()\n",
    "agent = sim.initialize_agent(0, agent_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Initial placement for agent and sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_node = sim.get_agent(0).body.object\n",
    "sensor_node = sim._sensors[\"rgba_camera\"]._sensor_object.object\n",
    "\n",
    "# initial agent transform\n",
    "agent_node.translation = [-0.15, -1.5, 1.0]\n",
    "agent_node.rotation = mn.Quaternion.rotation(mn.Deg(-75), mn.Vector3(0.0, 1.0, 0))\n",
    "\n",
    "# initial sensor local transform (relative to agent)\n",
    "sensor_node.translation = [0.0, 0.6, 0.0]\n",
    "sensor_node.rotation = mn.Quaternion.rotation(mn.Deg(-15), mn.Vector3(1.0, 0.0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Start an episode by moving an agent in the scene and capturing observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observations = []\n",
    "\n",
    "# simulate with empty scene\n",
    "observations += simulate_with_moving_agent(\n",
    "    sim,\n",
    "    duration=1.0,\n",
    "    agent_vel=np.array([0.5, 0.0, 0.0]),\n",
    "    look_rotation_vel=25.0,\n",
    "    get_frames=make_video_during_sim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Continue the episode by adding and simulating objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obj_templates_mgr = sim.get_object_template_manager()\n",
    "# get the rigid object manager, which provides direct\n",
    "# access to objects\n",
    "rigid_obj_mgr = sim.get_rigid_object_manager()\n",
    "\n",
    "obj_templates_mgr.load_configs(str(os.path.join(data_path, \"objects/example_objects\")))\n",
    "chefcan_template_handle = obj_templates_mgr.get_template_handles(\n",
    "    \"data/objects/example_objects/chefcan\"\n",
    ")[0]\n",
    "\n",
    "# drop some dynamic objects\n",
    "chefcan_1 = rigid_obj_mgr.add_object_by_template_handle(chefcan_template_handle)\n",
    "chefcan_1.translation = [2.4, -0.64, 0.0]\n",
    "chefcan_2 = rigid_obj_mgr.add_object_by_template_handle(chefcan_template_handle)\n",
    "chefcan_2.translation = [2.4, -0.64, 0.28]\n",
    "chefcan_3 = rigid_obj_mgr.add_object_by_template_handle(chefcan_template_handle)\n",
    "chefcan_3.translation = [2.4, -0.64, -0.28]\n",
    "\n",
    "observations += simulate_with_moving_agent(\n",
    "    sim,\n",
    "    duration=2.0,\n",
    "    agent_vel=np.array([0.0, 0.0, -0.4]),\n",
    "    look_rotation_vel=-5.0,\n",
    "    get_frames=make_video_during_sim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Continue the episode, removing some objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rigid_obj_mgr.remove_object_by_id(chefcan_1.object_id)\n",
    "rigid_obj_mgr.remove_object_by_id(chefcan_2.object_id)\n",
    "\n",
    "observations += simulate_with_moving_agent(\n",
    "    sim,\n",
    "    duration=2.0,\n",
    "    agent_vel=np.array([0.4, 0.0, 0.0]),\n",
    "    look_rotation_vel=-10.0,\n",
    "    get_frames=make_video_during_sim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## End the episode. Render the episode observations to a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if make_video_during_sim:\n",
    "    vut.make_video(\n",
    "        observations,\n",
    "        \"rgba_camera\",\n",
    "        \"color\",\n",
    "        output_path + \"episode\",\n",
    "        open_vid=show_video,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Write a replay to file and do some cleanup.\n",
    "gfx replay files are written as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim.gfx_replay_manager.write_saved_keyframes_to_file(replay_filepath)\n",
    "assert os.path.exists(replay_filepath)\n",
    "\n",
    "rigid_obj_mgr.remove_all_objects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Reconfigure simulator for replay playback.\n",
    "Note call to gfx_replay_utils.make_backend_configuration_for_playback. Note that we don't specify a scene or stage when reconfiguring for replay playback. need_separate_semantic_scene_graph is generally set to False. If you're using a semantic sensor and replaying a scene that uses a separate semantic mesh (like an MP3D scene), set this to True. If in doubt, be aware there's a Habitat runtime warning that will always catch incorrect usage of this flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim.close()\n",
    "\n",
    "# use same agents/sensors from earlier, with different backend config\n",
    "playback_cfg = habitat_sim.Configuration(\n",
    "    gfx_replay_utils.make_backend_configuration_for_playback(\n",
    "        need_separate_semantic_scene_graph=False\n",
    "    ),\n",
    "    cfg.agents,\n",
    ")\n",
    "\n",
    "if not sim:\n",
    "    sim = habitat_sim.Simulator(playback_cfg)\n",
    "else:\n",
    "    sim.reconfigure(playback_cfg)\n",
    "\n",
    "configure_lighting(sim)\n",
    "\n",
    "agent_state = habitat_sim.AgentState()\n",
    "sim.initialize_agent(0, agent_state)\n",
    "\n",
    "agent_node = sim.get_agent(0).body.object\n",
    "sensor_node = sim._sensors[\"rgba_camera\"]._sensor_object.object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Place dummy agent with identity transform.\n",
    "For replay playback, we place a dummy agent at the origin and then transform the sensor using the \"sensor\" user transform stored in the replay. In the future, Habitat will offer a cleaner way to play replays without an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_node.translation = [0.0, 0.0, 0.0]\n",
    "agent_node.rotation = mn.Quaternion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Load our earlier saved replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "player = sim.gfx_replay_manager.read_keyframes_from_file(replay_filepath)\n",
    "assert player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Play the replay!\n",
    "Note call to player.set_keyframe_index. Note also call to player.get_user_transform. For this playback, we restore our sensor to the original sensor transform from the episode. In this way, we reproduce the same observations. Note this doesn't happen automatically when using gfx replay; you must position your agent, sensor, or camera explicitly when playing a replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observations = []\n",
    "print(\"play replay #0...\")\n",
    "for frame in range(player.get_num_keyframes()):\n",
    "    player.set_keyframe_index(frame)\n",
    "\n",
    "    (sensor_node.translation, sensor_node.rotation) = player.get_user_transform(\n",
    "        \"sensor\"\n",
    "    )\n",
    "\n",
    "    observations.append(sim.get_sensor_observations())\n",
    "\n",
    "if make_video:\n",
    "    vut.make_video(\n",
    "        observations,\n",
    "        \"rgba_camera\",\n",
    "        \"color\",\n",
    "        output_path + \"replay_playback1\",\n",
    "        open_vid=show_video,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Play the replay again, in reverse at 3x speed (skipping frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observations = []\n",
    "print(\"play in reverse at 3x...\")\n",
    "for frame in range(player.get_num_keyframes() - 2, -1, -3):\n",
    "    player.set_keyframe_index(frame)\n",
    "    (sensor_node.translation, sensor_node.rotation) = player.get_user_transform(\n",
    "        \"sensor\"\n",
    "    )\n",
    "    observations.append(sim.get_sensor_observations())\n",
    "\n",
    "if make_video:\n",
    "    vut.make_video(\n",
    "        observations,\n",
    "        \"rgba_camera\",\n",
    "        \"color\",\n",
    "        output_path + \"replay_playback2\",\n",
    "        open_vid=show_video,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Play from a different camera view, with the original agent and sensor visualized using primitives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observations = []\n",
    "print(\"play from a different camera view, with agent/sensor visualization...\")\n",
    "\n",
    "# place a third-person camera\n",
    "sensor_node.translation = [-1.1, -0.9, -0.2]\n",
    "sensor_node.rotation = mn.Quaternion.rotation(mn.Deg(-115), mn.Vector3(0.0, 1.0, 0))\n",
    "\n",
    "# gather the agent trajectory for later visualization\n",
    "agent_trajectory_points = []\n",
    "for frame in range(player.get_num_keyframes()):\n",
    "    player.set_keyframe_index(frame)\n",
    "    (agent_translation, _) = player.get_user_transform(\"agent\")\n",
    "    agent_trajectory_points.append(agent_translation)\n",
    "\n",
    "debug_line_render = sim.get_debug_line_render()\n",
    "debug_line_render.set_line_width(2.0)\n",
    "agent_viz_box = mn.Range3D(mn.Vector3(-0.1, 0.0, -0.1), mn.Vector3(0.1, 0.4, 0.1))\n",
    "sensor_viz_box = mn.Range3D(mn.Vector3(-0.1, -0.1, -0.1), mn.Vector3(0.1, 0.1, 0.1))\n",
    "\n",
    "for frame in range(player.get_num_keyframes()):\n",
    "    player.set_keyframe_index(frame)\n",
    "\n",
    "    (agent_translation, agent_rotation) = player.get_user_transform(\"agent\")\n",
    "\n",
    "    rot_mat = agent_rotation.to_matrix()\n",
    "    full_mat = mn.Matrix4.from_(rot_mat, agent_translation)\n",
    "\n",
    "    # draw a box in the agent body's local space\n",
    "    debug_line_render.push_transform(full_mat)\n",
    "    debug_line_render.draw_box(\n",
    "        agent_viz_box.min, agent_viz_box.max, mn.Color4(1.0, 0.0, 0.0, 1.0)\n",
    "    )\n",
    "    debug_line_render.pop_transform()\n",
    "\n",
    "    for (radius, opacity) in [(0.2, 0.6), (0.25, 0.4), (0.3, 0.2)]:\n",
    "        debug_line_render.draw_circle(\n",
    "            agent_translation, radius, mn.Color4(0.0, 1.0, 1.0, opacity)\n",
    "        )\n",
    "\n",
    "    # draw a box in the sensor's local space\n",
    "    (sensor_translation, sensor_rotation) = player.get_user_transform(\"sensor\")\n",
    "    debug_line_render.push_transform(\n",
    "        mn.Matrix4.from_(sensor_rotation.to_matrix(), sensor_translation)\n",
    "    )\n",
    "    debug_line_render.draw_box(\n",
    "        sensor_viz_box.min, sensor_viz_box.max, mn.Color4(1.0, 0.0, 0.0, 1.0)\n",
    "    )\n",
    "    # draw a line in the sensor look direction (-z in local space)\n",
    "    debug_line_render.draw_transformed_line(\n",
    "        mn.Vector3.zero_init(),\n",
    "        mn.Vector3(0.0, 0.0, -0.5),\n",
    "        mn.Color4(1.0, 0.0, 0.0, 1.0),\n",
    "        mn.Color4(1.0, 1.0, 1.0, 1.0),\n",
    "    )\n",
    "    debug_line_render.pop_transform()\n",
    "\n",
    "    # draw the agent trajectory\n",
    "    debug_line_render.draw_path_with_endpoint_circles(\n",
    "        agent_trajectory_points, 0.07, mn.Color4(1.0, 1.0, 1.0, 1.0)\n",
    "    )\n",
    "\n",
    "    observations.append(sim.get_sensor_observations())\n",
    "\n",
    "if make_video:\n",
    "    vut.make_video(\n",
    "        observations,\n",
    "        \"rgba_camera\",\n",
    "        \"color\",\n",
    "        output_path + \"replay_playback3\",\n",
    "        open_vid=show_video,\n",
    "    )\n",
    "\n",
    "\n",
    "# clean up the player\n",
    "player.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Load multiple replays and create a \"sequence\" image.\n",
    "In this tutorial, we only recorded one replay. In general, you can load and play multiple replays and they are rendered \"additively\" (all objects from all replays are visualized on top of each other). Here, let's load multiple copies of our replay and create a single image showing different snapshots in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observations = []\n",
    "num_copies = 30\n",
    "other_players = []\n",
    "for i in range(num_copies):\n",
    "    other_player = sim.gfx_replay_manager.read_keyframes_from_file(replay_filepath)\n",
    "    assert other_player\n",
    "    other_player.set_keyframe_index(\n",
    "        other_player.get_num_keyframes() // (num_copies - 1) * i\n",
    "    )\n",
    "    other_players.append(other_player)\n",
    "\n",
    "# place a third-person camera\n",
    "sensor_node.translation = [1.0, -0.9, -0.3]\n",
    "sensor_node.rotation = mn.Quaternion.rotation(mn.Deg(-115), mn.Vector3(0.0, 1.0, 0))\n",
    "\n",
    "# Create a video by repeating this image a few times. This is a workaround because\n",
    "# we don't have make_image available in viz_utils. TODO: add make_image to\n",
    "# viz_utils.\n",
    "obs = sim.get_sensor_observations()\n",
    "for _ in range(10):\n",
    "    observations.append(obs)\n",
    "\n",
    "if make_video:\n",
    "    vut.make_video(\n",
    "        observations,\n",
    "        \"rgba_camera\",\n",
    "        \"color\",\n",
    "        output_path + \"replay_playback4\",\n",
    "        open_vid=show_video,\n",
    "    )\n",
    "\n",
    "# clean up the players\n",
    "for other_player in other_players:\n",
    "    other_player.close()\n",
    "\n",
    "# clean up replay file\n",
    "os.remove(replay_filepath)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Replay Tutorial",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "nb_python//py:percent,colabs//ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
